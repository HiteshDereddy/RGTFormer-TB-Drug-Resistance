# RGTFormer: Categorical Gated Transformer + Relational Graph Convolutional Network  
### Predicting Mutation-Associated Multi-Drug Resistance in *Mycobacterium tuberculosis*

**Authors:**  
Rakesh Chandra JoshiÂ¹, Hitesh Reddy DereddyÂ², Sandip MukhopadhyayÂ³, Radim Burgetâ´, Malay Kishore DuttaÂ¹*  
Â¹ Amity Centre for Artificial Intelligence, Amity University, Noida, India  
Â² Department of Artificial Intelligence, Amity School of Engineering & Technology, Amity University, Noida, India  
Â³ ICMRâ€“National Institute for Research in Bacterial Infections, Kolkata, India  
â´ Brno University of Technology, Czech Republic  
*Corresponding Author: malaykishoredutta@gmail.com*  

---

## ğŸ§¬ Overview
**RGTFormer** integrates a **Categorical Gated Transformer (CGT)** with a **Relational Graph Convolutional Network (RGCN)** to predict mutation-associated resistance to first- and second-line anti-tuberculosis (TB) drugs.  
It learns from both **sequence-based categorical** and **structure-based numerical** mutation descriptors to model multi-gene, multi-drug resistance with interpretability and efficiency.

---

## ğŸ“š Abstract
Tuberculosis (TB), caused by *Mycobacterium tuberculosis*, remains a global health threat, worsened by multi-drug-resistant TB (MDR-TB). Resistance often stems from single-nucleotide mutations in key genes.  
RGTFormer fuses RGCN-based relational reasoning and Transformer-based categorical attention to predict resistance from mutation profiles.  
Evaluated on 753 curated mutations across six genes (*rpoB, katG, inhA, pncA, gyrA, gyrB*), the model achieved:

- **Independent-test accuracy:** 98.67 %  
- **10-fold CV accuracy:** 97.15 %  
- **Precision:** 100 %â€ƒ**Recall:** 97.37 %â€ƒ**F1:** 98.67 %

---

## ğŸ§  Key Contributions
- ğŸ§© **Hybrid architecture** combining relational (RGCN) and categorical (CGT) feature learning.  
- ğŸ”¬ Handles heterogeneous genomic features â€” physicochemical + residue/structure.  
- ğŸ§® Attention-based fusion ensures interpretability and robustness.  
- ğŸ§« Multi-gene, multi-drug generalization across six resistance genes.  
- ğŸ“ˆ Outperforms all classical ML and deep-learning baselines.  
- ğŸ’¡ SHAP analysis reveals biologically meaningful feature contributions.

---

## ğŸ§© Dataset
| Gene | Drug | Variants | Resistant | Susceptible | Source |
|------|------|-----------|-----------|--------------|---------|
| rpoB | Rifampicin | 114 | 50 | 64 | TBDReaMDB + GMTV |
| katG | Isoniazid | 250 | 135 | 115 | TBDReaMDB + GMTV |
| inhA | Isoniazid | 27 | 10 | 17 | TBDReaMDB + GMTV |
| pncA | Pyrazinamide | 241 | 139 | 102 | TBDReaMDB + GMTV |
| gyrA | Fluoroquinolones | 72 | 31 | 41 | TBDReaMDB + GMTV |
| gyrB | Fluoroquinolones | 49 | 28 | 21 | TBDReaMDB + GMTV |

**Total:** 753 mutationsâ€ƒ**Labels:** Î”Î”G < 0 â†’ Resistant; Î”Î”G â‰¥ 0 â†’ Susceptible  

---

## âš™ï¸ Feature Engineering
**Numerical (z-score normalized mutantâ€“wild-type differences):**
1. Molecular weightâ€ƒ2. Van der Waals volumeâ€ƒ3. Polarity  
4. Isoelectric pointâ€ƒ5. Hydrophobicityâ€ƒ6. Normalized ASA  

**Categorical:**
- Residue type (0 charged / 1 polar / 2 aromatic / 3 hydrophobic)  
- Secondary structure (1 helix / 2 sheet / 3 coil / 4 turn)

---

## ğŸ§® Methodology

### ğŸ”— Workflow
![Fig 1 â€“ Workflow](figures/workflow.jpg)

### ğŸ§  Model Architecture
RGTFormer = RGCN + CGT + Attention Fusion  
![Fig 2 â€“ Architecture](figures/architecture.jpg)

**RGCN layer**
\begin{equation*}
h_i^{(l+1)} = \sigma\!\left(\sum_{r\in R}\sum_{j\in N_i^r}\!\tfrac{1}{c_{i,r}}W_r^{(l)}h_j^{(l)}\right)
\end{equation*}

![Fig 3 â€“ RGCN](figures/rgc.jpg)

**CGT (Gated Transformer)**
\[
z=ReLU(Wx+b),\quad GLU(z,g)=z\!\cdot\!\sigma(g)
\]
\[m=Softmax(Wx+b),\quad x'_i=x_i\odot m \]
![Fig 4 â€“ CGT](figures/cgt.jpg)

**Fusion + Classification**
\[
h=\alpha h_{RGCN}+\beta h_{CGT},\;\alpha+\beta=1,\quad
y=Softmax(Wh+b)
\]

**Training:** Adam (Î²â‚ = 0.9, Î²â‚‚ = 0.999), 100 epochs, BCE loss, PyTorch on NVIDIA A100 (40 GB)

---

## ğŸ§ª Experimental Results
| Variant | GNN Dim | Layers | LR | CGT Dim | Accuracy % | Precision % | Recall % | F1 % |
|:--|--:|--:|--:|--:|--:|--:|--:|--:|
| V1 | 64 | 3 | 0.001 | 64 | 97.35 | 98.65 | 96.05 | 97.33 |
| **V2 (Best)** | **128** | **4** | **0.0005** | **128** | **98.67** | **100.0** | **97.37** | **98.67** |
| V3 | 32 | 2 | 0.002 | 32 | 94.70 | 97.22 | 92.11 | 94.59 |

### Ablation & Baselines
| Model | 10-Fold Acc % | Test Acc % | Prec | Rec | F1 |
|:--|--:|--:|--:|--:|--:|
| **RGTFormer Full** | **97.15** | **98.67** | 1.00 | 0.97 | 0.99 |
| RGCN only | 94.57 | 96.02 | 0.97 | 0.95 | 0.96 |
| CGT only | 67.81 | 68.87 | 0.69 | 0.68 | 0.69 |
| GCN + Vanilla Transformer | 65.36 | 66.89 | 0.72 | 0.55 | 0.63 |

### Classical ML Comparison
| SVM | RF | Extra Trees | XGBoost | KNN | MLP (Complex) | ANN | **RGTFormer** |
|--|--|--|--|--|--|--|--|
| 90.07 | 93.38 | 92.72 | 95.36 | 95.36 | 94.70 | 94.04 | **98.67** |

**ROC AUC = 0.9923â€ƒ(149 / 151 correct)**  
| Confusion Matrix | ROC Curve |
|:--:|:--:|
| ![CM](results/cm.jpg) | ![ROC](results/roc.jpg) |

---

## ğŸ§© Explainability â€“ SHAP
![Fig 6 â€“ SHAP Importance](results/shap.jpg)  
Hydrophobicity â‰« Molecular Weight â‰« Isoelectric Point â‰« ASA as key features.

---

## ğŸ“Š Appendix Summary
| Feature | Mean | Std | Min | Max |
|--|--:|--:|--:|--:|
| Mol Weight | âˆ’0.0013 | 0.2989 | âˆ’1 | 0.77 |
| Volume | 0.005 | 0.2917 | âˆ’1 | 0.8 |
| Polarity | âˆ’0.0277 | 0.549 | âˆ’1 | 1 |
| pI | 0.0232 | 0.2956 | âˆ’0.71 | 0.87 |
| Hydrophobicity | âˆ’0.0456 | 0.3268 | âˆ’0.92 | 0.92 |
| Norm ASA | 0.4797 | 0.2721 | 0 | 1 |


### Correlation Matrix
![Corr](results/crm.jpg)

---

## ğŸ“š Comparison with Existing Studies
| Study | Method | Drugs | Accuracy | Remarks |
|--|--|--|--:|--|
| Hadikurniawati 2023 | Classical ML | RIF INH PZA EMB | ~99 | No graph features |
| CRyPTIC 2022 | ML on WGS | 13 | >95 | Low interpretability |
| Jamal 2020 | ML + Docking | 4 | ~85 | High runtime |
| Bhaskar 2023 | CNN on CT + Genomics | â€“ | 97.27 | No mutation-level |
| **RGTFormer** | CGT + RGCN | 6 | **98.67** | Interpretable + efficient |

---

## ğŸ’¡ Key Takeaways
- Hybrid graph-transformer captures both relational + contextual dependencies.  
- Mutation-level reasoning enhances interpretability.  
- SHAP reveals biologically coherent physicochemical drivers.  
- Extensible to other antimicrobial resistance tasks.

---

## ğŸ§° Implementation
**Environment**
```bash
Python >=3.10
PyTorch >=2.1
scikit-learn pandas numpy shap matplotlib networkx
